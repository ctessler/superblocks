\section{Related Work}\label{sec:related}

\subsection {CRPD Related Work}\label{sec:crpd_related_work}
Analyzing the preempted task, Lee et al.~\cite{lee:96}~\cite{lee:97}~\cite{lee:98} introduced the concept and algorithm for computing the set of useful cache blocks (UCB) for statically addressed instruction and data for direct mapped and set associative caches.  Here, dynamic memory accesses are treated as cache misses.  The UCBs of the preempted task are used to compute the cache related preemption delay (CRPD), which is simply the cardinality of the UCB set times the cache block reload time (BRT).  Analyzing the preempting task, Tomiyamay and Dutt~\cite{tomiyamay:00} computed the set of evicting cache blocks (ECBs) via program path analysis formulated in an integer linear programming (ILP) model for direct mapped instruction caches.  The set of UCBs and ECBs can be computed using control flow graph (CFG) analysis of reaching memory blocks (RMB) and live memory blocks (LMB) as in~\cite{lee:96}~\cite{lee:97}~\cite{lee:98}. In a similar fashion, the ECBs of the preempting task are used to compute the cache related preemption delay (CRPD), which is again the cardinality of the ECB set times the cache block reload time (BRT). Formal definitions of useful cache blocks (UCBs) and evicting cache blocks (ECBs) were outlined in~\cite{altmeyer:11c} by Altmeyer and Burguiere.  In essence, the preempting task’s memory accesses quantified as the set of evicting cache blocks will evict a useful cache blocks thereby imposing non-negligible cache related preemption delay (CRPD) on the preempted task.  Similar work by Negi et al.~\cite{negi:03} and Tan and Mooney~\cite{tan:04} compute the intersection of the ECB and UCB sets to achieve tighter bounds on the cache related preemption delay (CRPD) computation for direct-mapped and set-associative instruction caches. Negi’s work computes the set of UCBs and ECBs using the notion of reaching cache state (RCS) and live cache state (LCS). This representation of the cache content enhances Lee’s original approach at the cost of higher computational complexity. Staschulat and Ernst~\cite{staschulat:05c} realized an improvement in computational complexity at the expense of CRPD accuracy or tightness via a cache state reduction technique for direct mapped instruction caches that was later extended to address set-associative caches. This is accomplished by limiting the cache state size and selecting a revised cache state between two sets that minimizes the set difference.  One of the limitations with the existing UCB based analysis methods is their representation of memory blocks that may reside in cache memory.  This over-approximation was employed to realize a safe bounds on cache related preemption delay (CRPD) which is referred to as “may cache” in~\cite{altmeyer:11c}.  Likewise, worst case execution time (WCET) analysis tools use an over-approximation to estimate cache misses and an under-approximation to estimate cache hits.  Cache hits are memory blocks that must reside in cache memory hence the term used to describe this set is “must cache”~\cite{altmeyer:11c}.  Altmeyer and Burguiere ~\cite{altmeyer:11c} addressed this issue by introducing the notion of definitely-cached useful cache block (DC-UCB).  A memory block m is called a definitely-cached useful cache block (DC-UCB) at program point P, if (a) m must be cached at P, and, (b) m may be reused at program point Q that may be reached from P and must be cached along the path to its reuse~\cite{altmeyer:11c}.  The DC-UCB is useful in schedulability analysis to avoid double counting of cache misses resulting from intra-task cache block eviction.  Ramaprasad and Mueller~\cite{ramaprasad:06} examine the problem of dynamic addressing supporting cache related preemption delay (CRPD) analysis. Their algorithm employs memory access patterns to compute CRPD, instead of UCBs.  An important distinction to note is that instruction memory accesses are tightly coupled to the control flow graph in contrast to data memory accesses. This isomorphic property means the set of UCBs corresponding to data memory accesses changes more frequently thereby mandating the UCB computation at the instruction level.

\subsection {Limited Preemption Related Work}\label{sec:lp_related_work}
The motivation for limited preemption scheduling approaches stems from limitations of fully preemptive and non-preemptive scheduling.  Fully preemptive scheduling suffers from schedulability degradation due to increased preemption overhead penalties which cache related preemption delay (CRPD) comprises a significant portion.  Non-preemptive scheduling suffers from reduced task utilization due to the blocking imposed on high priority jobs.  These factors have motivated research on alternative limited preemption scheduling approaches with the goal of achieving higher task utilization and reduced preemption overhead.  One such approach is known as the deferred preemption model.  The basic idea behind the deferred preemption model is to permit a currently executing job to execute non-preemptively for some period of time after the arrival of high priority job.  Two distinct models of deferred preemption have been proposed by Burns~\cite{burns:05} and Baruah~\cite{baruah:05} known as fixed preemption point model and floating preemption point model respectively.  In the floating preemption point model~\cite{baruah:05}, the beginning non-preemptive regions occur with the arrival of a higher priority job.  The currently executing job continues executing non-preemptively for \begin{math}Q_{i}\end{math} time or earlier if the job completes execution. In essence, Baruah’s approach computes the maximum amount of blocking time denoted \begin{math}Q_{i}\end{math} for which a task \begin{math}\tau_{i}\end{math} may execute non-preemptively while still preserving scheduling feasibility. The location of the non-preemptive regions is nondeterministic or essentially floating.  Another limited preemption scheduling technique known as preemption threshold scheduling (PTS) was proposed by Wang and Saksena in~\cite{wang:99}.  In preemption threshold scheduling, each task is assigned two priority values, namely, a nominal static priority \begin{math}p_{i}\end{math} and a preemption threshold \begin{math}\Pi_{i}\end{math}.  A task will be preempted only if the preempting task has a nominal priority \begin{math}p_{i}\end{math} greater than the preemption threshold \begin{math}\Pi_{i}\end{math}. In the fixed preemption point model ~\cite{burns:05}, a task can be preempted only at a limited set of pre-defined locations. Basically, tasks contain a series of non-preemptive regions.  Preemptions are permitted at non-preemptive region boundaries or fixed preemption points. Two closely related derivative works implementing a fixed preemption point model for a fixed priority (FP) scheduler were proposed by Simonson and Patel in~\cite{simonson:95} and by Lee et. al. in~\cite{lee:98} whose objective was to reduce preemption overhead.  Simonson’s and Patel’s approach in~\cite{simonson:95}, tasks are sub-divided into distinct non-overlapping intervals, each constrained by the maximum blocking time \begin{math}Q_{i}\end{math} that higher priority tasks may be subjected to while preserving task set schedulability.  At a location within each interval containing the minimum number of useful cache blocks (UCBs), a single preemption point is placed.  Lee et. al. in~\cite{lee:98} adopted a different method whereby the locations preemption points are commensurate with the cardinality of the useful cache block set less than a pre-determined threshold.   While both techniques serve to improve preemption overhead over the fully preemptive approach, their heuristic nature is unable to achieve a globally optimal solution.  In contrast, Bertogna et. al. in~\cite{bertogna:10} achieved an optimal preemption point placement algorithm with polynomial time complexity.  The analysis assumed a pessimistic fixed constant context switch cost at each preemption point equal to the largest preemption overhead experienced by a task.  Later work by Bertogna et. al. in~\cite{bertogna:11} relaxes the fixed constant context switch assumption using more accurate variable preemption overhead cost information via available timing analysis tools.  Our work improves these results by computing the cache related preemption delay (CRPD) contribution to the variable preemption overhead cost as a function of the current and next selected preemption point.  Our method not only accounts for the cache blocks evicted due to preemption, but also accounts for the cache blocks that are reloaded during execution between preemption points thereby improving the accuracy of marginal cache related preemption delay (CRPD) computations for multiple nested preemptions.  Due to the variability in cache related preemption delay (CRPD) and preemption overhead cost, we propose a dynamic programming algorithm to realize a globally optimal preemption point placement that further minimizes the number of preemptions and the preemption overhead cost due to the increased CRPD accuracy.  The dynamic programming algorithm assumes a linear basic block structure with high level programming language constructs fully contained within the basic block.  The need to subsume higher level programming constructs being the prominent assumption of the linear basic block structure can potentially diminish the utility of our approach if the non-preemptive execution time of any basic block violates the constraint \begin{math}q_{BB}\end{math} \begin{math}>\end{math} \begin{math}Q_{i}\end{math}. In future work, we plan to relax the linear basic block structure assumption and address arbitrarily connected basic block structures.

~\cite{altmeyer:11} ~\cite{hanninen:11} ~\cite{yao:10} ~\cite{altmeyer:12} ~\cite{altmeyer:11b} ~\cite{marinho:12} ~\cite{bastoni:11} ~\cite{marinho:11} ~\cite{buttazzo:13} ~\cite{negi:03} ~\cite{staschulat:05} ~\cite{tomiyamay:00} ~\cite{staschulat:04} ~\cite{altmeyer:09} ~\cite{ju:07} ~\cite{ramaprasad:06} ~\cite{bertogna:10} ~\cite{sebek:01}  ~\cite{bertogna:11}  ~\cite{staschulat:05b}  ~\cite{staschulat:05c}  ~\cite{zhang:07} ~\cite{ramaprasad:08} ~\cite{schliecker:10} ~\cite{ramaprasad:10}  ~\cite{staschulat:04b}  ~\cite{lv:10}  ~\cite{chakraborty:09} ~\cite{kleinsorge:11}  ~\cite{ramaprasad:09}  ~\cite{schliecker:10b} ~\cite{mezzetti:10} ~\cite{lunniss:13} ~\cite{davis:06} ~\cite{zhang:12} ~\cite{lunniss:12} ~\cite{altmeyer:10b}  ~\cite{chattopadhyay:13}  ~\cite{puaut:09} ~\cite{peng:13}  ~\cite{prete:09} ~\cite{marinho:12b} ~\cite{davis:12} ~\cite{lee:96} ~\cite{tan:04}  ~\cite{lee:98}  ~\cite{lee:97} ~\cite{bastoni:10}  ~\cite{davis:13} ~\cite{petters:01} ~\cite{lee:01} ~\cite{ramaprasad:06b} ~\cite{busquets-mataix:96} ~\cite{hardy:09} ~\cite{dobrin:04} ~\cite{yao:10b} ~\cite{altmeyer:09b}   ~\cite{davis:13b} ~\cite{vera:03} ~\cite{bertogna:11b} ~\cite{marinho:12c} ~\cite{bertogna:11c}  ~\cite{yao:11} ~\cite{marinho:13} ~\cite{marinho:12d} ~\cite{yao:09} ~\cite{buttazzo:13} ~\cite{wilhelm:08}  ~\cite{baldovin:13} ~\cite{davis:13c} ~\cite{schneider:00} ~\cite{staschulat:06} ~\cite{puaut:06} ~\cite{chakraborty:11} ~\cite{burns:05} ~\cite{baruah:05} ~\cite{wang:99} ~\cite{simonson:95}  ~\cite{altmeyer:11c} ~\cite{pellizzoni:07} ~\cite{pellizzoni:08} ~\cite{pellizzoni:11} 